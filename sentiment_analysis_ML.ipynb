{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "import jieba  \n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaocjia/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Movie_Name_EN</th>\n",
       "      <th>Movie_Name_CN</th>\n",
       "      <th>Crawl_Date</th>\n",
       "      <th>Number</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Star</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Like</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movie_Name_CN</th>\n",
       "      <th>Star</th>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">七月与安生</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>1012184</th>\n",
       "      <td>Soulmate</td>\n",
       "      <td>七月与安生</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>7152</td>\n",
       "      <td>陈信宏大过天</td>\n",
       "      <td>2016-10-13</td>\n",
       "      <td>0</td>\n",
       "      <td>也许是七月 也许是安生 总有一颗不想稳定下来的心</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034976</th>\n",
       "      <td>Soulmate</td>\n",
       "      <td>七月与安生</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>30357</td>\n",
       "      <td>砂糖的砂</td>\n",
       "      <td>2016-11-07</td>\n",
       "      <td>0</td>\n",
       "      <td>跟全世界路过差不多水平吧 摄影也有很大问题 全片抓不出来一帧称得上电影构图的画面 剧情理解...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Movie_Name_EN Movie_Name_CN  Crawl_Date  Number  \\\n",
       "Movie_Name_CN Star ID                                                        \n",
       "七月与安生         0    1012184      Soulmate         七月与安生  2017-01-05    7152   \n",
       "                   1034976      Soulmate         七月与安生  2017-01-05   30357   \n",
       "\n",
       "                           Username        Date  Star  \\\n",
       "Movie_Name_CN Star ID                                   \n",
       "七月与安生         0    1012184   陈信宏大过天  2016-10-13     0   \n",
       "                   1034976     砂糖的砂  2016-11-07     0   \n",
       "\n",
       "                                                                      Comment  \\\n",
       "Movie_Name_CN Star ID                                                           \n",
       "七月与安生         0    1012184                           也许是七月 也许是安生 总有一颗不想稳定下来的心   \n",
       "                   1034976   跟全世界路过差不多水平吧 摄影也有很大问题 全片抓不出来一帧称得上电影构图的画面 剧情理解...   \n",
       "\n",
       "                            Like  \n",
       "Movie_Name_CN Star ID             \n",
       "七月与安生         0    1012184     0  \n",
       "                   1034976     0  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('DMSC.csv', index_col=0)\n",
    "data = data.assign(Star=data['Star'].map(lambda x: 0 if x <=3 else 1))\n",
    "sample_df = data.groupby(['Movie_Name_CN', 'Star']).apply(\n",
    "    lambda x: x.sample(n=int(2125056/(28*100)), replace=True, random_state=0))\n",
    "sample_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33958, 8490, 33958, 8490)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = sample_df.values[:, 7]\n",
    "star = sample_df.values[:, 6]\n",
    "\n",
    "x_train, x_test, y_train, y_test, = train_test_split(comments, star, test_size=0.2, random_state=0)\n",
    "\n",
    "len(y_train), len(y_test), len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "1. 清理非中文字符\n",
    "2. 结巴分词\n",
    "3. 去除停用词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理非中文字符\n",
    "def clean_str(line):\n",
    "    line.strip('\\n')\n",
    "    line = re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", line)\n",
    "    line = re.sub(\n",
    "        \"[0-9a-zA-Z\\-\\s+\\.\\!\\/_,$%^*\\(\\)\\+(+\\\"\\')]+|[+——！，。？、~@#￥%……&*（）<>\\[\\]:：★◆【】《》;；=?？]+\", \"\", line)\n",
    "    return line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词\n",
    "with open('stopWord.txt') as f:\n",
    "    stopwords = [line.strip('\\n') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(text_data, labels, stopwords):\n",
    "    result = []\n",
    "    new_labels = []\n",
    "    for index in tqdm_notebook(range(len(text_data))):\n",
    "        comment = clean_str(text_data[index])\n",
    "        label = labels[index]\n",
    "        # 分词\n",
    "        seg_list = jieba.cut(comment, cut_all=False, HMM=True)\n",
    "        seg_list = [x.strip('\\n')\n",
    "                    for x in seg_list if x not in stopwords and len(x) > 1]\n",
    "        if len(seg_list) > 1:\n",
    "            result.append(\" \".join(seg_list))\n",
    "            new_labels.append(label)\n",
    "    # 返回分词结果和对应的标签\n",
    "    return result, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9089cc8d0add44d58c9afc523e84cd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=33958), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48bcf8f3c0e434a870160bbdf235f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8490), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 分别对训练数据和测试数据进行数据预处理\n",
    "train_cut_result, train_labels = cut(x_train, y_train, stopwords)\n",
    "test_cut_result, test_labels = cut(x_test, y_test, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算TF-IDF\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words=None)\n",
    "X_train = vectorizer.fit_transform(train_cut_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_cut_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features: 34898\n",
      "features: ['一一', '一丁点', '一万', '一万个', '一万倍', '一万年', '一万次', '一万遍', '一上', '一上午']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "print(\"n_features: %d\" % len(feature_names))\n",
    "print(\"features: %s\" % feature_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 100 best features by a chi-squared test\n",
      "done in 0.063143s\n"
     ]
    }
   ],
   "source": [
    "# 卡方检验，特征选择。选100个。\n",
    "select_chi2 = 100\n",
    "print(\"Extracting %d best features by a chi-squared test\" % select_chi2)\n",
    "t0 = time()\n",
    "ch2 = SelectKBest(chi2, k=select_chi2)\n",
    "X_train = ch2.fit_transform(X_train, train_labels)\n",
    "X_test = ch2.transform(X_test)\n",
    "\n",
    "feature_names = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "print(\"done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report = True\n",
    "target_names = [\"积极\", \"消极\"]\n",
    "\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, train_labels)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(test_labels, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    \n",
    "    print(\"top 20 keywords per class:\")\n",
    "    top20 = np.argsort(clf.coef_[0])[-20:]\n",
    "    end20 = np.argsort(clf.coef_[0])[:20]\n",
    "    print(trim(\"%s: %s\" % (\"积极\", \" \".join([feature_names[i] for i in top20]))))\n",
    "    print(trim(\"%s: %s\" % (\"消极\", \" \".join([feature_names[i] for i in end20]))))\n",
    "\n",
    "    if print_report:\n",
    "        print(\"classification report:\")\n",
    "        print(metrics.classification_report(test_labels, pred,\n",
    "                                            target_names=target_names))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.015s\n",
      "test time:  0.001s\n",
      "accuracy:   0.637\n",
      "top 20 keywords per class:\n",
      "积极: 支持 爱情 精彩 值得 很棒 五星 感动 国产 我们 什么 非常 很多 虽然 青春 一部 没有 剧情 喜欢 好看 不错\n",
      "消极: 虚高 侮辱 最烂 负分 玩意 空洞 看不下去 浪费 乱七八糟 两颗 无力 太烂 圈钱 垃圾 小品 三分 混乱 无感 新意 毫无\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          积极       0.73      0.43      0.54      3767\n",
      "          消极       0.60      0.84      0.70      3831\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      7598\n",
      "   macro avg       0.66      0.64      0.62      7598\n",
      "weighted avg       0.66      0.64      0.62      7598\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.011s\n",
      "test time:  0.001s\n",
      "accuracy:   0.636\n",
      "top 20 keywords per class:\n",
      "积极: 很棒 不能 那些 值得 五星 爱情 国产 非常 感动 我们 青春 很多 什么 虽然 一部 没有 好看 喜欢 剧情 不错\n",
      "消极: 虚高 侮辱 最烂 负分 玩意 两颗 浪费 看不下去 乱七八糟 空洞 太烂 小品 三分 无力 垃圾 圈钱 混乱 一般般 无感 力荐\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          积极       0.60      0.78      0.68      3767\n",
      "          消极       0.69      0.50      0.58      3831\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      7598\n",
      "   macro avg       0.65      0.64      0.63      7598\n",
      "weighted avg       0.65      0.64      0.63      7598\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "ComplementNB(alpha=0.1, class_prior=None, fit_prior=True, norm=False)\n",
      "train time: 0.010s\n",
      "test time:  0.001s\n",
      "accuracy:   0.637\n",
      "top 20 keywords per class:\n",
      "积极: 五星 最烂 侮辱 泪点 满分 震撼 不管 能量 完美 挺不错 过瘾 很赞 值得一看 二刷 轻松 很爽 大爱 五分 淡淡的 力荐\n",
      "消极: 剧情 没有 什么 不错 好看 一般 喜欢 一星 一部 不能 尴尬 烂片 三星 完全 实在 除了 睡着 失望 台词 无聊\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          积极       0.60      0.78      0.68      3767\n",
      "          消极       0.70      0.49      0.58      3831\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      7598\n",
      "   macro avg       0.65      0.64      0.63      7598\n",
      "weighted avg       0.65      0.64      0.63      7598\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "results.append(benchmark(ComplementNB(alpha=.1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
